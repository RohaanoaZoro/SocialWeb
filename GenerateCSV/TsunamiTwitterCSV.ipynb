{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c581cfb0",
   "metadata": {},
   "source": [
    "# Generating CSV for Tsunami.\n",
    "\n",
    "## Method:\n",
    "1. Use twitter api to get data related to a #HashTag in a time period\n",
    "2. Extract relevant data(magnitude, depth, date, time) from the text retrieved\n",
    "3. Clean the data\n",
    "4. Convert data to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976292e",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Use twitter api to get data related to a HashTag in a time period\n",
    "\n",
    "1. After creating a developer account in twitter and applying for an academic licence we get the api_key and token to access the premium services of twitter api to get all the tweets related to a #hashtag\n",
    "\n",
    "2. We search for the #Tsunami to get all tweets related to tsunami but for our experiment we foucus on reliable tweets therefore we have only fetched tweets from the USGS twitter page by searching for @tsunamiwatch\n",
    "\n",
    "3. We can fine-tune our search by including the date till which to search. Our period of search was 1.1.2008 to 1.1.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c35fbda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# read configs\n",
    "config = {}\n",
    "\n",
    "api_key = \"gU5n2ENwKk5ZitQhnFJD4LhEF\"\n",
    "api_key_secret = \"dpqRaymjE1N3nHkQNA1k8cUo1GZtYvKoLARkBP0VjpnjymN9QW\"\n",
    "\n",
    "access_token = \"941059418346602496-ppmslALuGBqeNKFEZaxpCazONh7dExI\"\n",
    "access_token_secret = \"vX81wOWTg7hz486b5AbOftrMucN7Klx3buVjSZIqWfTTG\"\n",
    "\n",
    "# authentication\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "try:\n",
    "    api = tweepy.API(auth)\n",
    "    public_tweets = api.search_tweets(\"@tsunamiwatch\", until=\"2022-12-15\")\n",
    "except:\n",
    "    print(\"API key, secret or token is missing or incorrect\") \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de20b1",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Extract relevant data from the text retrieved\n",
    "\n",
    "The text information contains a lot of information, much of which is unnecessary. Therefore we use regular expression matching to only retieve those results that are relevant to us. \n",
    "\n",
    "Example:\n",
    "For tsuami we look for data such as magnitude, depth, maximum water height, latitude, longitude and date-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90aefaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(text):\n",
    "\n",
    "    m = re.search('M[0-9][.][0-9]', text)\n",
    "    d = re.search('depth [0-9][.][0-9]', text)\n",
    "    lat = re.search('(\\d*[.]\\d)°[N|S]', text)\n",
    "    long = re.search('(\\d*[.]\\d)°[E|W]', text)\n",
    "    h = re.search('height [0-9][.][0-9]m', text)\n",
    "    l = re.search('\\((.*), (.*)\\)', text)\n",
    "    \n",
    "    url = re.search('(.*)', text)\n",
    "    \n",
    "    magnitude=\"\"\n",
    "    depth=\"\"\n",
    "    latitude=\"\"\n",
    "    longitude=\"\"\n",
    "    location = \"\"\n",
    "    country = \"\"\n",
    "    maxwaterheight = \"\"\n",
    "    \n",
    "    if m:\n",
    "        magnitude =  m.group(0)\n",
    "        \n",
    "    if lat:\n",
    "        latitude = lat.group(1)\n",
    "        \n",
    "    if long:\n",
    "        longitude = long.group(1)\n",
    "        \n",
    "    if d:\n",
    "        depth = d.group(1)\n",
    "        \n",
    "    if h:\n",
    "        maxwaterheight = h.group(1)\n",
    "        \n",
    "    if l:\n",
    "        location = l.group(1)\n",
    "        country = l.group(2)\n",
    "        \n",
    "        \n",
    "    return [magnitude, depth, latitude, longitude, country, location, maxwaterheight]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960aa24",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Convert the data to a dataframe\n",
    "\n",
    "After regular expression matching we have successfully extracted the relevant data.\n",
    "\n",
    "We now combine the data we recieved from the twitter headers along with the data extracted from the tweet text and store it in a data frame.\n",
    "\n",
    "if some information is empty then its is due to a changes in twitter's api that has truncated the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ce63980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>EarthquakeMagnitude</th>\n",
       "      <th>Deposits</th>\n",
       "      <th>Country</th>\n",
       "      <th>Location Name</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MaximumWaterHeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-14 20:41:01+00:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-14 19:53:07+00:00</td>\n",
       "      <td>M6.1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>51.6</td>\n",
       "      <td>178.6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-14 19:07:58+00:00</td>\n",
       "      <td>M6.1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>51.6</td>\n",
       "      <td>178.6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-14 16:49:19+00:00</td>\n",
       "      <td>M5.1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43.9</td>\n",
       "      <td>128.1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-14 13:13:48+00:00</td>\n",
       "      <td>M5.1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43.9</td>\n",
       "      <td>128.1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-12-14 13:05:21+00:00</td>\n",
       "      <td>M5.1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43.9</td>\n",
       "      <td>128.1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-12-14 13:02:27+00:00</td>\n",
       "      <td>M5.1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43.9</td>\n",
       "      <td>128.1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime EarthquakeMagnitude  Deposits Country  \\\n",
       "0 2022-12-14 20:41:01+00:00                             0           \n",
       "1 2022-12-14 19:53:07+00:00                M6.1         0           \n",
       "2 2022-12-14 19:07:58+00:00                M6.1         0           \n",
       "3 2022-12-14 16:49:19+00:00                M5.1         0           \n",
       "4 2022-12-14 13:13:48+00:00                M5.1         0           \n",
       "5 2022-12-14 13:05:21+00:00                M5.1         0           \n",
       "6 2022-12-14 13:02:27+00:00                M5.1         0           \n",
       "\n",
       "  Location Name Latitude Longitude MaximumWaterHeight  \n",
       "0                                                      \n",
       "1                   51.6     178.6                     \n",
       "2                   51.6     178.6                     \n",
       "3                   43.9     128.1                     \n",
       "4                   43.9     128.1                     \n",
       "5                   43.9     128.1                     \n",
       "6                   43.9     128.1                     "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe\n",
    "columns = ['Datetime','EarthquakeMagnitude','Deposits','Country','Location Name','Latitude','Longitude', 'MaximumWaterHeight']\n",
    "data = []\n",
    "for tweet in public_tweets:\n",
    "    reldata = getData(tweet.text)\n",
    "    data.append([tweet.created_at, reldata[0], 0,  reldata[4], reldata[5], reldata[2], reldata[3], reldata[6]])\n",
    "    \n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdbfdf",
   "metadata": {},
   "source": [
    "## 4. Convert data to CSV\n",
    "\n",
    "We use one of python's built-in functions to convert a datafram to CSV file. This CSV file will be used for further processing later as mentioned in the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tsunami_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
